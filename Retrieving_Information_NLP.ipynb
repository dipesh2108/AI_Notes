{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dipesh2108/AI_Notes/blob/main/Retrieving_Information_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdivP0pVKXx_"
      },
      "source": [
        "Deep Learning for NLP\n",
        "--\n",
        "\n",
        "Recipe 7-1:  Retrieving Information\n",
        "--\n",
        "\n",
        "Information retrieval is one of the highly used applications of NLP and it is\n",
        "quite tricky. The meaning of the words or sentences not only depends on the exact words used but also on the context and meaning. Two sentences may be of completely different words but can convey the same meaning. We should be able to capture that as well.\n",
        "\n",
        "An information retrieval (IR) system allows users to efficiently search documents and retrieve meaningful information based on a search text/query.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1QOtLrYLecQSPH3D_SgaFg12urVyeOu3b\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDR47qnjNDXX"
      },
      "source": [
        "Problem\n",
        "--\n",
        "Information retrieval using word embeddings.\n",
        "\n",
        "Solution\n",
        "--\n",
        "There are multiple ways to do Information retrieval. But we will see how to do it using word embeddings, which is very effective since it takes context also into consideration.\n",
        "\n",
        "We will just use the pretrained word2vec in this case.\n",
        "( Src : https://radimrehurek.com/gensim/models/word2vec.html )\n",
        "\n",
        "Let’s take a simple example and see how to build a document retrieval using query input. Let’s say we have 4 documents in our database as below. (Just showcasing how it works. We will have too many documents in a real-world application.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-DEUUdqKvwD_"
      },
      "source": [
        "Doc1 = [\"With the Union cabinet approving the amendments to the Motor Vehicles Act, 2016, those caught for drunken driving will have to have really deep pockets, as the fine payable in court has been enhanced to Rs 10,000 for first-time offenders.\" ]\n",
        "\n",
        "Doc2 = [\"Natural language processing (NLP) is an area of computer science and artificial intelligence concerned with the interactions between computers and human (natural) languages, in particular how to program computers to process and analyze large amounts of natural language data.\"]\n",
        "\n",
        "Doc3 = [\"He points out that public transport is very good in Mumbai and New Delhi, where there is a good network of suburban and metro rail systems.\"]\n",
        "\n",
        "Doc4 = [\"But the man behind the wickets at the other end was watching just as keenly. With an affirmative nod from Dhoni, India captain Rohit Sharma promptly asked for a review. Sure enough, the ball would have clipped the top of middle and leg.\"]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iTAj0OMNU8J"
      },
      "source": [
        "Assume we have numerous documents like this. And you want to retrieve the most relevant one, for the query “cricket.” Let’s see how to build it.\n",
        "\n",
        "**query = \"cricket\"**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SzD4v887kZ69",
        "outputId": "807a0999-4544-45e0-912e-4b840578b31c"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-FWGfWSqQru",
        "outputId": "964db5ca-9386-42cc-abfe-3785436edc6f"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mH0XAMIJkOhm"
      },
      "source": [
        "# Step 7.1.1 : Import the libraries\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "import numpy as np\n",
        "import nltk\n",
        "import itertools\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import scipy\n",
        "from scipy import spatial\n",
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "import re\n",
        "\n",
        "tokenizer = ToktokTokenizer()\n",
        "stopword_list = nltk.corpus.stopwords.words('english')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VGtk76Juke11",
        "outputId": "977eb31c-90e5-44ca-9d93-ad637ed9b1c1"
      },
      "source": [
        "# Step 7.1.2 - Create/import documents\n",
        "\n",
        "# Doc1 , Doc2 , Doc3 and Doc4 as defined above in the code.\n",
        "\n",
        "# Put all the documents in one list\n",
        "fin = Doc1+Doc2+Doc3+Doc4\n",
        "print(fin)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['With the Union cabinet approving the amendments to the Motor Vehicles Act, 2016, those caught for drunken driving will have to have really deep pockets, as the fine payable in court has been enhanced to Rs 10,000 for first-time offenders.', 'Natural language processing (NLP) is an area of computer science and artificial intelligence concerned with the interactions between computers and human (natural) languages, in particular how to program computers to process and analyze large amounts of natural language data.', 'He points out that public transport is very good in Mumbai and New Delhi, where there is a good network of suburban and metro rail systems.', 'But the man behind the wickets at the other end was watching just as keenly. With an affirmative nod from Dhoni, India captain Rohit Sharma promptly asked for a review. Sure enough, the ball would have clipped the top of middle and leg.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vclGFshLkoeU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b186c4c7-ba07-48c0-c1f1-e515b7135957"
      },
      "source": [
        "# import gensim package\n",
        "#import gensim\n",
        "\n",
        "# load the saved model\n",
        "#model = gensim.models.KeyedVectors.load_word2vec_format('http://d1ufe8q8sjuo99.cloudfront.net/public/GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
        "## http://d1ufe8q8sjuo99.cloudfront.net/public/GoogleNews-vectors-negative300.bin.gz\n",
        "\n",
        "import gensim.downloader as api\n",
        "wv = api.load('word2vec-google-news-300')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcOTFB-CstWE"
      },
      "source": [
        "#print(model)\n",
        "#print(gensim.models.Word2Vec())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVCsU5zDlPJ0"
      },
      "source": [
        "# Step 7.1.4 : Create IR system\n",
        "\n",
        "# Now we build the information retrieval system:\n",
        "\n",
        "# Preprocessing\n",
        "def remove_stopwords(text, is_lower_case=False):\n",
        " pattern = r'[^a-zA-Z0-9\\s]'\n",
        " text = re.sub(pattern, \"\" , text)\n",
        "\n",
        "\n",
        " tokens = tokenizer.tokenize(text)\n",
        " tokens = [token.strip() for token in tokens]\n",
        " if is_lower_case:\n",
        "  filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
        "\n",
        " else:\n",
        "  filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
        "  filtered_text = ' '.join(filtered_tokens)\n",
        "\n",
        " return filtered_text\n",
        "\n",
        "\n",
        "# Function to get the embedding vector for n dimension, we have used \"300\"\n",
        "def get_embedding(word):\n",
        " if word in wv.key_to_index:\n",
        "  return wv[word]\n",
        " else:\n",
        "  return np.zeros(300)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7v65UrBvct_d",
        "outputId": "789e07c6-3725-4e5a-b13b-cbc972129414",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# just to see the word vector for some word.\n",
        "print(wv['cricket'])\n",
        "print(\"-----------------------------------\")\n",
        "print(len(wv['cricket']))\n",
        "print(np.mean(np.array(wv['cricket']), axis=0))\n",
        "\n",
        "# just to see no. of words in Doc1\n",
        "print(len(tokenizer.tokenize(Doc1)))\n",
        "\n",
        "\n",
        "# so, we would get 47 word vectors, each of size 300.\n",
        "# One vector for one word.\n",
        "# so, we could get the mean of each vector\n",
        "# so that we reduce of the values we have to handle"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-3.67187500e-01 -1.21582031e-01  2.85156250e-01  8.15429688e-02\n",
            "  3.19824219e-02 -3.19824219e-02  1.34765625e-01 -2.73437500e-01\n",
            "  9.46044922e-03 -1.07421875e-01  2.48046875e-01 -6.05468750e-01\n",
            "  5.02929688e-02  2.98828125e-01  9.57031250e-02  1.39648438e-01\n",
            " -5.41992188e-02  2.91015625e-01  2.85156250e-01  1.51367188e-01\n",
            " -2.89062500e-01 -3.46679688e-02  1.81884766e-02 -3.92578125e-01\n",
            "  2.46093750e-01  2.51953125e-01 -9.86328125e-02  3.22265625e-01\n",
            "  4.49218750e-01 -1.36718750e-01 -2.34375000e-01  4.12597656e-02\n",
            " -2.15820312e-01  1.69921875e-01  2.56347656e-02  1.50146484e-02\n",
            " -3.75976562e-02  6.95800781e-03  4.00390625e-01  2.09960938e-01\n",
            "  1.17675781e-01 -4.19921875e-02  2.34375000e-01  2.03125000e-01\n",
            " -1.86523438e-01 -2.46093750e-01  3.12500000e-01 -2.59765625e-01\n",
            " -1.06933594e-01  1.04003906e-01 -1.79687500e-01  5.71289062e-02\n",
            " -7.41577148e-03 -5.59082031e-02  7.61718750e-02 -4.14062500e-01\n",
            " -3.65234375e-01 -3.35937500e-01 -1.54296875e-01 -2.39257812e-01\n",
            " -3.73046875e-01  2.27355957e-03 -3.51562500e-01  8.64257812e-02\n",
            "  1.26953125e-01  2.21679688e-01 -9.86328125e-02  1.08886719e-01\n",
            "  3.65234375e-01 -5.66406250e-02  5.66406250e-02 -1.09375000e-01\n",
            " -1.66992188e-01 -4.54101562e-02 -2.00195312e-01 -1.22558594e-01\n",
            "  1.31835938e-01 -1.31835938e-01  1.03027344e-01 -3.41796875e-01\n",
            " -1.57226562e-01  2.04101562e-01  4.39453125e-02  2.44140625e-01\n",
            " -3.19824219e-02  3.20312500e-01 -4.41894531e-02  1.08398438e-01\n",
            " -4.98046875e-02 -9.52148438e-03  2.46093750e-01 -5.59082031e-02\n",
            "  4.07714844e-02 -1.78222656e-02 -2.95410156e-02  1.65039062e-01\n",
            "  5.03906250e-01 -2.81250000e-01  9.81445312e-02  1.80664062e-02\n",
            " -1.83593750e-01  2.53906250e-01  2.25585938e-01  1.63574219e-02\n",
            "  1.81640625e-01  1.38671875e-01  3.33984375e-01  1.39648438e-01\n",
            "  1.45874023e-02 -2.89306641e-02 -8.39843750e-02  1.50390625e-01\n",
            "  1.67968750e-01  2.28515625e-01  3.59375000e-01  1.22558594e-01\n",
            " -3.28125000e-01 -1.56250000e-01  2.77343750e-01  1.77001953e-02\n",
            " -1.46484375e-01 -4.51660156e-03 -4.46777344e-02  1.75781250e-01\n",
            " -3.75000000e-01  1.16699219e-01 -1.39648438e-01  2.55859375e-01\n",
            " -1.96289062e-01 -2.57568359e-02 -5.41992188e-02 -2.51464844e-02\n",
            " -1.93359375e-01 -3.17382812e-02 -8.74023438e-02 -1.32812500e-01\n",
            " -2.12402344e-02  4.33593750e-01 -5.20019531e-02  3.46679688e-02\n",
            "  8.00781250e-02  3.41796875e-02  1.99218750e-01 -2.39257812e-02\n",
            " -2.37304688e-01  1.93359375e-01  7.32421875e-02 -2.87109375e-01\n",
            "  1.25000000e-01  8.44726562e-02  1.30859375e-01 -2.19726562e-01\n",
            " -1.61132812e-01 -2.63671875e-01 -5.46875000e-01 -2.96875000e-01\n",
            "  3.44238281e-02 -2.87109375e-01 -1.93359375e-01 -1.61132812e-01\n",
            " -3.84765625e-01 -2.14843750e-01 -6.22558594e-03 -1.27929688e-01\n",
            " -1.00097656e-01 -6.21093750e-01  3.78906250e-01 -4.58984375e-01\n",
            "  1.44531250e-01 -9.13085938e-02 -3.08593750e-01  2.23632812e-01\n",
            "  7.86132812e-02 -2.16796875e-01  8.78906250e-02 -1.66992188e-01\n",
            "  1.14746094e-02 -2.53906250e-01 -6.25000000e-02  6.04248047e-03\n",
            "  1.56250000e-01  4.37500000e-01 -2.23632812e-01 -2.32421875e-01\n",
            "  2.75390625e-01  2.39257812e-01  4.49218750e-02 -7.51953125e-02\n",
            "  5.74218750e-01 -2.61230469e-02 -1.21582031e-01  2.44140625e-01\n",
            " -3.37890625e-01  8.59375000e-02 -7.71484375e-02  4.85839844e-02\n",
            "  1.43554688e-01  4.25781250e-01 -4.29687500e-02 -1.08398438e-01\n",
            "  1.19628906e-01 -1.91406250e-01 -2.12890625e-01 -2.87109375e-01\n",
            " -1.14746094e-01 -2.04101562e-01 -2.06298828e-02 -2.53906250e-01\n",
            "  8.25195312e-02 -3.97949219e-02 -1.57226562e-01  1.34765625e-01\n",
            "  2.08007812e-01 -1.78710938e-01 -2.00195312e-02 -8.34960938e-02\n",
            " -1.20605469e-01  4.29687500e-02 -1.94335938e-01 -1.32812500e-01\n",
            " -2.17285156e-02 -2.35351562e-01 -3.63281250e-01  1.51367188e-01\n",
            "  9.32617188e-02  1.63085938e-01  1.02050781e-01 -4.27734375e-01\n",
            "  2.83203125e-01  2.74658203e-04 -3.20312500e-01  1.68457031e-02\n",
            "  4.06250000e-01 -5.24902344e-02  7.91015625e-02 -1.41601562e-01\n",
            "  5.27343750e-01 -1.26953125e-01  4.74609375e-01 -6.64062500e-02\n",
            "  3.41796875e-01 -1.78710938e-01  3.69140625e-01 -2.05078125e-01\n",
            "  5.82885742e-03 -1.84570312e-01 -8.88671875e-02 -1.81640625e-01\n",
            " -4.80957031e-02  4.39453125e-01  2.12890625e-01 -3.07617188e-02\n",
            "  9.32617188e-02  2.40234375e-01  2.39257812e-01  2.51953125e-01\n",
            " -1.98974609e-02  1.24511719e-01 -4.73632812e-02 -2.13623047e-02\n",
            "  3.12500000e-02  3.05175781e-02  2.79296875e-01  9.08203125e-02\n",
            " -2.02148438e-01 -2.19726562e-02 -2.63671875e-01  8.78906250e-02\n",
            " -1.07421875e-01 -2.49023438e-01 -1.22070312e-02  1.73828125e-01\n",
            " -9.91210938e-02  7.27539062e-02  2.59765625e-01 -4.60937500e-01\n",
            "  3.59375000e-01 -2.25585938e-01  1.87988281e-02 -2.19726562e-01\n",
            " -2.08984375e-01 -1.51367188e-01  8.64257812e-02  1.11694336e-02\n",
            "  6.93359375e-02 -2.99072266e-02  1.43554688e-01  1.89453125e-01\n",
            " -1.32812500e-01  4.72656250e-01 -1.40625000e-01 -2.52685547e-02\n",
            "  1.91406250e-01 -2.63671875e-01 -1.39648438e-01  1.09375000e-01\n",
            "  1.97753906e-02  2.49023438e-01 -1.42578125e-01  4.15039062e-02]\n",
            "-----------------------------------\n",
            "300\n",
            "-0.0021000672\n",
            "47\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UzUaICTknvhN",
        "outputId": "e23ab720-ea41-4536-a940-4fef85672a94",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Getting average vector for each document\n",
        "out_dict = {}\n",
        "for sen in fin:  # this loop will pick one sentence at a time from fin (final document)\n",
        " average_vector = (np.mean(np.array([get_embedding(x) for x in nltk.word_tokenize(remove_stopwords(sen))]), axis=0))\n",
        " dict = { sen : (average_vector) }\n",
        " out_dict.update(dict)\n",
        "\n",
        "# Function to calculate the similarity between the query vector and document vector\n",
        "def get_sim(query_embedding, average_vector_doc):\n",
        " sim = [(1 - scipy.spatial.distance.cosine(query_embedding, average_vector_doc))]\n",
        " return sim"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VpMhsCBsJaC"
      },
      "source": [
        "print(out_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOoYnJNbvRAE"
      },
      "source": [
        "# Rank all the documents based on the similarity to get relevant docs\n",
        "def Ranked_documents(query):\n",
        " query_words = (np.mean(np.array([get_embedding(x) for x in nltk.word_tokenize(query.lower())],dtype=float), axis=0))\n",
        " rank = []\n",
        "\n",
        " for k,v in out_dict.items():\n",
        "  rank.append((k, get_sim(query_words, v)))\n",
        "\n",
        " rank = sorted(rank,key=lambda t: t[1], reverse=True)\n",
        " print('Ranked Documents :')\n",
        " return rank"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnmo1x-_v5VI",
        "outputId": "b31fc4eb-1cc9-4d6e-f549-2ce80370c479",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Call the IR function with a query\n",
        "Ranked_documents(\"cricket\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ranked Documents :\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('But the man behind the wickets at the other end was watching just as keenly. With an affirmative nod from Dhoni, India captain Rohit Sharma promptly asked for a review. Sure enough, the ball would have clipped the top of middle and leg.',\n",
              "  [0.4495432862154046]),\n",
              " ('He points out that public transport is very good in Mumbai and New Delhi, where there is a good network of suburban and metro rail systems.',\n",
              "  [0.23973446434982593]),\n",
              " ('With the Union cabinet approving the amendments to the Motor Vehicles Act, 2016, those caught for drunken driving will have to have really deep pockets, as the fine payable in court has been enhanced to Rs 10,000 for first-time offenders.',\n",
              "  [0.1832371201201335]),\n",
              " ('Natural language processing (NLP) is an area of computer science and artificial intelligence concerned with the interactions between computers and human (natural) languages, in particular how to program computers to process and analyze large amounts of natural language data.',\n",
              "  [0.17995060864421153])]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6Odmi7DwFSJ",
        "outputId": "51902492-d4b4-44ed-f86c-6d6b46aecd0a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Let’s take one more example as may be driving.\n",
        "Ranked_documents(\"driving is cool on National Highways\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ranked Documents :\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('He points out that public transport is very good in Mumbai and New Delhi, where there is a good network of suburban and metro rail systems.',\n",
              "  [0.49226976071196904]),\n",
              " ('With the Union cabinet approving the amendments to the Motor Vehicles Act, 2016, those caught for drunken driving will have to have really deep pockets, as the fine payable in court has been enhanced to Rs 10,000 for first-time offenders.',\n",
              "  [0.47816491997287947]),\n",
              " ('But the man behind the wickets at the other end was watching just as keenly. With an affirmative nod from Dhoni, India captain Rohit Sharma promptly asked for a review. Sure enough, the ball would have clipped the top of middle and leg.',\n",
              "  [0.3475713893546848]),\n",
              " ('Natural language processing (NLP) is an area of computer science and artificial intelligence concerned with the interactions between computers and human (natural) languages, in particular how to program computers to process and analyze large amounts of natural language data.',\n",
              "  [0.29431722277161776])]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7l7KmO2Nsre"
      },
      "source": [
        "We can use the same approach and scale it up for as many documents as possible.\n",
        "\n",
        "For more accuracy, we can build our own embeddings, for specific industries since the one we are using is generalized.\n",
        "(Although it is time consuming task)\n",
        "\n",
        "This is the fundamental approach that can be used for many applications like the following:\n",
        "1. Search engines (Internet Search Engines)\n",
        "2. Document retrieval\n",
        "3. Passage retrieval\n",
        "4. Question and answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kp-cl9AmNwcE"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1kGARF7WUBYrDKbRkQh5l7V01pDTYCwiU\">\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mh7yMxVqOeAO"
      },
      "source": [
        "It’s been proven that results will be good when queries are longer and\n",
        "the result length is shorter. That’s the reason we don’t get great results in search engines when the search query has lesser number of words."
      ]
    }
  ]
}