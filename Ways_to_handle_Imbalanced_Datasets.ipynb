{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dipesh2108/AI_Notes/blob/main/Ways_to_handle_Imbalanced_Datasets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Handling imbalanced datasets is an important aspect of many machine learning tasks, especially when the distribution of classes in the dataset is skewed, and one class significantly outnumbers the others. There are several techniques to address imbalanced datasets. Here's an overview with pseudo code examples:\n",
        "\n",
        "1. **Resampling Methods:**\n",
        "\n",
        "   a. **Oversampling the Minority Class:** You can generate additional samples for the minority class to balance the class distribution.\n",
        "\n",
        "   ```python\n",
        "   # Pseudo code for oversampling the minority class\n",
        "   from sklearn.utils import resample\n",
        "\n",
        "   # Separate the majority and minority classes\n",
        "   majority_class = data[data['target'] == 'majority']\n",
        "   minority_class = data[data['target'] == 'minority']\n",
        "\n",
        "   # Oversample the minority class to match the majority class size\n",
        "   minority_class_oversampled = resample(minority_class, replace=True, n_samples=len(majority_class), random_state=42)\n",
        "   # When replace=True, it indicates that sampling is done with replacement.\n",
        "   # This means that each time a data point is randomly selected from the minority class\n",
        "   # to be included in the oversampled dataset, it is not removed from the original minority class dataset.\n",
        "   # Therefore, the same data point can be selected multiple times in the oversampling process,\n",
        "   # potentially leading to duplicated data points in the oversampled dataset.\n",
        "\n",
        "   # Combine the majority class and oversampled minority class\n",
        "   balanced_data = pd.concat([majority_class, minority_class_oversampled])\n",
        "   ```\n",
        "\n",
        "   b. **Undersampling the Majority Class:** You can reduce the size of the majority class to match the minority class size.\n",
        "\n",
        "   ```python\n",
        "   # Pseudo code for undersampling the majority class\n",
        "   from sklearn.utils import resample\n",
        "\n",
        "   # Separate the majority and minority classes\n",
        "   majority_class = data[data['target'] == 'majority']\n",
        "   minority_class = data[data['target'] == 'minority']\n",
        "\n",
        "   # Undersample the majority class to match the minority class size\n",
        "   majority_class_undersampled = resample(majority_class, replace=False, n_samples=len(minority_class), random_state=42)\n",
        "   # When replace=False, each sample from the majority class is selected exactly once to form the undersampled dataset.\n",
        "   # As a result, the number of samples in the undersampled majority class\n",
        "   # will be less than or equal to the number of samples in the original minority class.\n",
        "\n",
        "   # Combine the minority class and undersampled majority class\n",
        "   balanced_data = pd.concat([minority_class, majority_class_undersampled])\n",
        "   ```\n",
        "\n",
        "2. **Synthetic Data Generation:**\n",
        "\n",
        "   a. **SMOTE (Synthetic Minority Over-sampling Technique):** SMOTE generates synthetic samples for the minority class by interpolating between existing minority class samples.\n",
        "\n",
        "   ```python\n",
        "   # Pseudo code for SMOTE\n",
        "   from imblearn.over_sampling import SMOTE\n",
        "\n",
        "   # Instantiate SMOTE\n",
        "   smote = SMOTE(random_state=42)\n",
        "\n",
        "   # Apply SMOTE to the dataset\n",
        "   X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "   ```\n",
        "\n",
        "3. **Ensemble Methods:**\n",
        "\n",
        "   a. **Balanced Random Forest:** Use an ensemble technique like Balanced Random Forest, which balances class weights to improve classification on imbalanced datasets.\n",
        "\n",
        "   ```python\n",
        "   # Pseudo code for Balanced Random Forest\n",
        "   from imblearn.ensemble import BalancedRandomForestClassifier\n",
        "\n",
        "   # Instantiate Balanced Random Forest\n",
        "   brf = BalancedRandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "   # Fit the model\n",
        "   brf.fit(X_train, y_train)\n",
        "   ```\n",
        "\n",
        "4. **Cost-sensitive Learning:**\n",
        "\n",
        "   a. **Assign Different Costs:** Modify the classification algorithm to consider different misclassification costs for each class.\n",
        "\n",
        "   ```python\n",
        "   # Pseudo code for cost-sensitive learning with different misclassification costs\n",
        "   from sklearn.svm import SVC\n",
        "\n",
        "   # Specify class weights to assign different misclassification costs\n",
        "   class_weights = {0: 1, 1: 10}  # Example: Class 1 is 10 times more costly to misclassify\n",
        "\n",
        "   # Instantiate the SVM classifier with class weights\n",
        "   svm_classifier = SVC(class_weight=class_weights)\n",
        "\n",
        "   # Fit the classifier\n",
        "   svm_classifier.fit(X_train, y_train)\n",
        "   ```\n",
        "\n",
        "5. **Evaluation Metrics:**\n",
        "\n",
        "   a. **Use Appropriate Evaluation Metrics:** When dealing with imbalanced datasets, focus on evaluation metrics like precision, recall, F1-score, and area under the ROC curve (AUC-ROC) rather than accuracy.\n",
        "\n",
        "   ```python\n",
        "   # Pseudo code for calculating precision and recall\n",
        "   from sklearn.metrics import precision_score, recall_score\n",
        "\n",
        "   y_pred = model.predict(X_test)\n",
        "\n",
        "   precision = precision_score(y_test, y_pred)\n",
        "   recall = recall_score(y_test, y_pred)\n",
        "   ```\n",
        "\n",
        "6. **Anomaly Detection:**\n",
        "\n",
        "   a. **Treat as an Anomaly Detection Problem:** In some cases, you can treat the minority class as anomalies and use anomaly detection techniques like isolation forests or one-class SVM.\n",
        "\n",
        "   ```python\n",
        "   # Pseudo code for using Isolation Forest for anomaly detection\n",
        "   from sklearn.ensemble import IsolationForest\n",
        "\n",
        "   # Instantiate Isolation Forest\n",
        "   isolation_forest = IsolationForest(contamination=0.1, random_state=42)\n",
        "\n",
        "   # Fit the model and classify anomalies\n",
        "   anomaly_labels = isolation_forest.fit_predict(X)\n",
        "   ```\n",
        "\n",
        "Handling imbalanced datasets requires a thoughtful approach based on the specific characteristics of your data and the problem you are solving. The choice of method depends on the nature of the dataset and the goals of your machine learning task."
      ],
      "metadata": {
        "id": "uyjxEKWifJoQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Quick Note on `contamination` parameter\n",
        "\n",
        "In the context of the `IsolationForest` algorithm, the `contamination` parameter represents the expected proportion of outliers (anomalies) in the dataset. It allows you to specify the approximate percentage of data points you expect to be outliers.\n",
        "\n",
        "Here's what it means:\n",
        "\n",
        "- `contamination`: This parameter is a float value typically ranging between 0 and 1, where:\n",
        "  - A value of 0 indicates that you expect no outliers in your dataset, assuming that the data follows a normal distribution.\n",
        "  - A value of 1 indicates that you expect all data points to be outliers.\n",
        "  - A value between 0 and 1 represents your expectation of the proportion of outliers in the dataset.\n",
        "\n",
        "The `IsolationForest` algorithm works by isolating outliers (anomalies) in a dataset, which are data points that are significantly different from the majority of the data. By setting the `contamination` parameter, you specify your expectation of the percentage of such outliers in your data.\n",
        "\n",
        "For example, if you set `contamination=0.1`, you are indicating that you expect approximately 10% of the data points to be outliers or anomalies. The `IsolationForest` algorithm will then aim to identify and isolate these data points as anomalies during the modeling process.\n",
        "\n",
        "The appropriate choice of the `contamination` parameter depends on your domain knowledge and the characteristics of your dataset. You may need to experiment with different values to find the one that best suits your specific anomaly detection task."
      ],
      "metadata": {
        "id": "p6nzJSpPjlNO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use Case - Credit Card Fraud Detection\n",
        "\n",
        "#### **Context**\n",
        "It is important that credit card companies are able to recognize fraudulent credit card transactions so that customers are not charged for items that they did not purchase.\n",
        "\n",
        "#### **Source of dataset**\n",
        "https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud\n",
        "\n",
        "#### **Content**\n",
        "The dataset contains transactions made by credit cards in September 2013 by European cardholders.\n",
        "This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n",
        "\n",
        "It contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data. Features V1, V2, … V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-sensitive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.\n",
        "\n"
      ],
      "metadata": {
        "id": "mCCyCIask35p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQ5S5CptfD-6",
        "outputId": "fb2044e1-2543-43e7-e5c7-f5fb1b8c73a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class distribution before SMOTE:\n",
            "Counter({0.0: 6951, 1.0: 24})\n",
            "\n",
            "Class distribution after SMOTE:\n",
            "Counter({0.0: 6951, 1.0: 6951})\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from collections import Counter\n",
        "\n",
        "# Load the Credit Card Fraud Detection dataset (replace with your dataset path)\n",
        "data = pd.read_csv(\"creditcard.csv\")\n",
        "\n",
        "# Separate features (X) and target variable (y)\n",
        "X = data.drop(\"Class\", axis=1)\n",
        "y = data[\"Class\"]\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Standardize the features (optional but recommended)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Instantiate the SMOTE oversampling technique\n",
        "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
        "\n",
        "# Apply SMOTE to the training data\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# Check the class distribution before and after oversampling\n",
        "print(\"Class distribution before SMOTE:\")\n",
        "print(Counter(y_train))\n",
        "\n",
        "print(\"\\nClass distribution after SMOTE:\")\n",
        "print(Counter(y_train_resampled))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training a classifier on both the original training data and the oversampled training data\n",
        "\n",
        "Let's train a classifier on both the original training data (X_train, y_train) and the oversampled training data (X_train_resampled, y_train_resampled) and compare their performance using the F1-score. We'll use a common classifier, such as the Random Forest Classifier.\n",
        "\n",
        "`Note` - You can replace it with other classifiers as needed."
      ],
      "metadata": {
        "id": "8rp2JyY2nOPa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import f1_score, classification_report\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Create and train a Random Forest Classifier on the original training data\n",
        "clf_original = RandomForestClassifier(random_state=42)\n",
        "clf_original.fit(X_train, y_train)\n",
        "\n",
        "# Create and train a Random Forest Classifier on the oversampled training data\n",
        "clf_oversampled = RandomForestClassifier(random_state=42)\n",
        "clf_oversampled.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# Preprocess the test data to handle missing values (NaN)\n",
        "imputer = SimpleImputer(strategy='mean')  # You can use other strategies like 'median' or 'most_frequent'\n",
        "X_test_imputed = imputer.fit_transform(X_test)\n",
        "\n",
        "# Ensure that y_test does not contain NaN values\n",
        "# If y_test contains NaN values, remove those rows from X_test and y_test\n",
        "nan_indices = np.isnan(y_test)\n",
        "X_test_cleaned = X_test_imputed[~nan_indices]\n",
        "y_test_cleaned = y_test[~nan_indices]\n",
        "\n",
        "# Make predictions on the cleaned test set using both classifiers\n",
        "y_pred_original = clf_original.predict(X_test_cleaned)\n",
        "y_pred_oversampled = clf_oversampled.predict(X_test_cleaned)\n",
        "\n",
        "# Calculate F1-score for both classifiers\n",
        "f1_score_original = f1_score(y_test_cleaned, y_pred_original)\n",
        "f1_score_oversampled = f1_score(y_test_cleaned, y_pred_oversampled)\n",
        "\n",
        "# Display F1-scores and classification reports for both classifiers\n",
        "print(\"F1-Score (Original Data):\", f1_score_original)\n",
        "print(\"F1-Score (Oversampled Data):\", f1_score_oversampled)\n",
        "\n",
        "print(\"\\nClassification Report (Original Data):\")\n",
        "print(classification_report(y_test_cleaned, y_pred_original))\n",
        "\n",
        "print(\"\\nClassification Report (Oversampled Data):\")\n",
        "print(classification_report(y_test_cleaned, y_pred_oversampled))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ryqwkMPemRkv",
        "outputId": "a4ae2a6f-d5ad-4e2b-ebf4-a089d6409ce8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1-Score (Original Data): 0.923076923076923\n",
            "F1-Score (Oversampled Data): 0.923076923076923\n",
            "\n",
            "Classification Report (Original Data):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      1.00      1.00      2975\n",
            "         1.0       1.00      0.86      0.92        14\n",
            "\n",
            "    accuracy                           1.00      2989\n",
            "   macro avg       1.00      0.93      0.96      2989\n",
            "weighted avg       1.00      1.00      1.00      2989\n",
            "\n",
            "\n",
            "Classification Report (Oversampled Data):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      1.00      1.00      2975\n",
            "         1.0       1.00      0.86      0.92        14\n",
            "\n",
            "    accuracy                           1.00      2989\n",
            "   macro avg       1.00      0.93      0.96      2989\n",
            "weighted avg       1.00      1.00      1.00      2989\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What does it mean on getting the same F1-Score ?\n",
        "\n",
        "If you're getting the same F1-score for both the original data and the oversampled data, it suggests that the oversampling technique (SMOTE) did not significantly impact the F1-score in this specific case. This can happen when the original classifier (trained on the imbalanced data) already performs well on the minority class, and oversampling didn't provide a substantial improvement.\n",
        "\n",
        "While the F1-score is a valuable metric for evaluating classifier performance, especially in imbalanced datasets, it might not always show a noticeable difference between the original and oversampled data, particularly when the original model performs well.\n",
        "\n",
        "In such cases, you might consider checking other performance metrics or conducting further analysis to evaluate the effectiveness of oversampling:\n",
        "\n",
        "1. **Precision and Recall**: Look at precision and recall separately. Oversampling often improves recall (true positive rate) at the cost of precision (how many of the predicted positives are true positives). Depending on your problem, you might prioritize one over the other.\n",
        "\n",
        "2. **ROC Curve and AUC**: Examine the receiver operating characteristic (ROC) curve and calculate the area under the ROC curve (AUC-ROC). This can help you assess the model's ability to discriminate between the classes.\n",
        "\n",
        "3. **Confusion Matrix**: Analyze the confusion matrix to understand false positives and false negatives in more detail. This can provide insights into where the improvements or trade-offs are happening.\n",
        "\n",
        "4. **Area Under the Precision-Recall Curve (AUC-PR)**: The AUC-PR summarizes the trade-off between precision and recall. A higher AUC-PR indicates a better balance between these two metrics.\n",
        "\n",
        "5. **Specificity**: Compute the specificity (true negative rate) in addition to sensitivity (true positive rate) to get a complete picture of classifier performance.\n",
        "\n",
        "6. **Cross-Validation**: Perform cross-validation to assess the generalization performance of the models on different subsets of the data. This can provide a more robust evaluation.\n",
        "\n",
        "7. **Grid Search and Hyperparameter Tuning**: Optimize hyperparameters of the classifier to see if better performance can be achieved.\n",
        "\n",
        "Remember that the choice of the evaluation metric depends on your specific problem and what you consider most important. In some cases, a small change in F1-score might not be the best indicator of improvement, and other metrics may provide a clearer picture of the model's performance."
      ],
      "metadata": {
        "id": "BHNd0qD6qUAk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import average_precision_score\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Create and train a Random Forest Classifier on the original training data\n",
        "clf_original = RandomForestClassifier(random_state=42)\n",
        "clf_original.fit(X_train, y_train)\n",
        "\n",
        "# Create and train a Random Forest Classifier on the oversampled training data\n",
        "clf_oversampled = RandomForestClassifier(random_state=42)\n",
        "clf_oversampled.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# Preprocess the test data to handle missing values (NaN)\n",
        "imputer = SimpleImputer(strategy='mean')  # You can use other strategies like 'median' or 'most_frequent'\n",
        "X_test_imputed = imputer.fit_transform(X_test)\n",
        "\n",
        "# Ensure that y_test does not contain NaN values\n",
        "nan_indices = np.isnan(y_test)\n",
        "X_test_cleaned = X_test_imputed[~nan_indices]\n",
        "y_test_cleaned = y_test[~nan_indices]\n",
        "\n",
        "# Make probability predictions on the cleaned test set using both classifiers\n",
        "y_prob_original = clf_original.predict_proba(X_test_cleaned)[:, 1]\n",
        "y_prob_oversampled = clf_oversampled.predict_proba(X_test_cleaned)[:, 1]\n",
        "\n",
        "# Calculate AUC-PR for both classifiers\n",
        "auc_pr_original = average_precision_score(y_test_cleaned, y_prob_original)\n",
        "auc_pr_oversampled = average_precision_score(y_test_cleaned, y_prob_oversampled)\n",
        "\n",
        "# Display AUC-PR for both classifiers\n",
        "print(\"AUC-PR (Original Data):\", auc_pr_original)\n",
        "print(\"AUC-PR (Oversampled Data):\", auc_pr_oversampled)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-4REDPxznmX7",
        "outputId": "85e18af5-97fb-42bb-dbce-1e6ddaf2c649"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUC-PR (Original Data): 0.9468253968253968\n",
            "AUC-PR (Oversampled Data): 0.9238039478086317\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## In this code:\n",
        "\n",
        "1. We use the predict_proba method to obtain probability predictions for both the original and oversampled classifiers. These probabilities are used to compute the AUC-PR.\n",
        "\n",
        "2. We calculate the AUC-PR for both classifiers using the average_precision_score function.\n",
        "\n",
        "3. We print the AUC-PR for both the original and oversampled data.\n",
        "\n",
        "> The AUC-PR provides insight into the model's ability to rank positive samples higher than negative samples, making it a useful metric for evaluating classifiers on imbalanced datasets."
      ],
      "metadata": {
        "id": "CS0DXocLrXqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Quick Note on predict_proba()\n",
        "\n",
        "```python\n",
        "clf_original.predict_proba(X_test_cleaned)[:, 1]\n",
        "```\n",
        "\n",
        "is used to obtain the predicted probabilities of the positive class (class 1) from a trained classifier (`clf_original` in this case) for a given test dataset (`X_test_cleaned`).\n",
        "\n",
        "Here's a breakdown of the code:\n",
        "\n",
        "1. `clf_original`: This is the trained Random Forest Classifier that you previously fit on the original training data.\n",
        "\n",
        "2. `.predict_proba(X_test_cleaned)`: This part of the code uses the `predict_proba` method of the classifier to compute the predicted probabilities for each class (in this case, there are two classes: 0 and 1) for the test dataset `X_test_cleaned`. The result is an array where each row corresponds to a sample in the test dataset, and each column corresponds to a class. Column 0 corresponds to class 0 (negative class), and column 1 corresponds to class 1 (positive class).\n",
        "\n",
        "3. `[:, 1]`: Finally, `[:, 1]` is used to select all rows (all samples) from column 1 of the predicted probabilities. This means that you are extracting the predicted probabilities of the positive class (class 1) for all samples in the test dataset.\n",
        "\n",
        "So, the line of code essentially gives you an array of predicted probabilities of the positive class for each sample in the test dataset. This is often used when you want to calculate metrics that depend on probability scores, such as the Area Under the Precision-Recall Curve (AUC-PR), which requires probability scores for positive class predictions.\n",
        "\n",
        "-----\n",
        "\n",
        "Connect with the author of this Notebook - [Rocky Jagtiani](https://www.linkedin.com/in/rocky-jagtiani-3b390649/)  \n",
        "\n",
        "-----"
      ],
      "metadata": {
        "id": "n-GCRUuIsZb0"
      }
    }
  ]
}