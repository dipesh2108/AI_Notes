{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dipesh2108/AI_Notes/blob/main/11_Converting_Text_to_Features.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0cozD-Xgcv6"
      },
      "source": [
        "Converting Text to Features Using One Hot Encoding\n",
        "--\n",
        "The traditional method used for feature engineering is One Hot encoding.\n",
        "If anyone knows the basics of machine learning, One Hot encoding is\n",
        "something they should have come across for sure at some point of time or\n",
        "maybe most of the time. It is a process of converting categorical variables\n",
        "into features or columns and coding one or zero for the presence of that\n",
        "particular category. We are going to use the same logic here, and the\n",
        "number of features is going to be the number of total tokens present in the\n",
        "whole corpus.\n",
        "\n",
        "**On hot encoding was covered under \"Machine Learning Course\" also."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTNQsWl_gcwE"
      },
      "source": [
        "Problem\n",
        "--\n",
        "You want to convert text to feature using One Hot encoding.\n",
        "\n",
        "Solution\n",
        "--\n",
        "One Hot Encoding will basically convert characters or words into binary\n",
        "numbers as shown below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ssyzIJL4gcwG",
        "outputId": "3a3434a7-b529-4c90-d446-1a6a2a11057f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       I    NLP     am    and    fun     is     it  learning\n",
              "0   True  False  False  False  False  False  False     False\n",
              "1  False  False   True  False  False  False  False     False\n",
              "2  False  False  False  False  False  False  False      True\n",
              "3  False   True  False  False  False  False  False     False\n",
              "4  False  False  False   True  False  False  False     False\n",
              "5  False  False  False  False  False  False   True     False\n",
              "6  False  False  False  False  False   True  False     False\n",
              "7  False  False  False  False   True  False  False     False"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-65d4f38a-9f23-4b6d-a176-bcc728b79ac6\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>I</th>\n",
              "      <th>NLP</th>\n",
              "      <th>am</th>\n",
              "      <th>and</th>\n",
              "      <th>fun</th>\n",
              "      <th>is</th>\n",
              "      <th>it</th>\n",
              "      <th>learning</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-65d4f38a-9f23-4b6d-a176-bcc728b79ac6')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-65d4f38a-9f23-4b6d-a176-bcc728b79ac6 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-65d4f38a-9f23-4b6d-a176-bcc728b79ac6');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-de6ab544-4894-44bc-9a92-5181829faef4\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-de6ab544-4894-44bc-9a92-5181829faef4')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-de6ab544-4894-44bc-9a92-5181829faef4 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"pd\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": \"I\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          false,\n          true\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"NLP\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          true,\n          false\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"am\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          true,\n          false\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"and\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          true,\n          false\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"fun\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          true,\n          false\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"is\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          true,\n          false\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"it\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          true,\n          false\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"learning\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          true,\n          false\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "Text = \"I am learning NLP and it is fun\"\n",
        "\n",
        "# Importing the library\n",
        "import pandas as pd\n",
        "\n",
        "# Generating the features\n",
        "pd.get_dummies(Text.split())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdA6wSTGgcwJ"
      },
      "source": [
        "Output has 7 features since the number of distinct words present in the\n",
        "input was 7."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yaE2p-atgcwK"
      },
      "source": [
        "Converting Text to Features Using Count Vectorizing\n",
        "--\n",
        "Above approach \"One hot encoding\" has a disadvantage. It does not take the\n",
        "frequency of the word occurring into consideration. If a particular word\n",
        "is appearing multiple times, there is a chance of missing the information\n",
        "if it is not included in the analysis. A count vectorizer will solve that\n",
        "problem.\n",
        "\n",
        "Problem\n",
        "--\n",
        "How do we convert text to feature using a count vectorizer?\n",
        "\n",
        "Solution\n",
        "--\n",
        "Count vectorizer is almost similar to One Hot encoding. The only\n",
        "difference is instead of checking whether the particular word is present or\n",
        "not, it will count the words that are present in the document.\n",
        "Observe the below example. The words “I” and “NLP” occur twice in\n",
        "the first document."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "eyQFha_cgcwL",
        "outputId": "a1a73f4e-50cb-4d21-cde4-9f52a99e0d9a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'love': 3, 'nlp': 4, 'and': 0, 'will': 6, 'learn': 2, 'in': 1, 'sessions': 5}\n",
            "[[1 1 1 1 2 1 1]]\n"
          ]
        }
      ],
      "source": [
        "#importing the function\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Text\n",
        "text = [\"I love nlp and I will learn NLP in 4 sessions\"]\n",
        "\n",
        "# create the CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# tokenizing\n",
        "vectorizer.fit(text)\n",
        "\n",
        "# encode document\n",
        "vector = vectorizer.transform(text)\n",
        "# counting is case in-sensitive\n",
        "\n",
        "# summarize & generating output\n",
        "print(vectorizer.vocabulary_)\n",
        "print(vector.toarray())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNWOW8D6gcwM"
      },
      "source": [
        "The fifth token nlp has appeared twice in the document.\n",
        "\n",
        "**Note** : CountVectorizer does not consider single char words like `I` , `a`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7OvqDn1gcwO"
      },
      "source": [
        "Generating N-grams ( also called Bag of words )\n",
        "--\n",
        "If you observe the above methods, each word is considered as a feature.\n",
        "There is a drawback to this method. It does not consider the previous and the next words, to see if that would give a proper and complete meaning to the words.\n",
        "\n",
        "For example: consider the word “not bad.” If this is split into individual\n",
        "words, then it will lose out on conveying “good” – which is what this word\n",
        "actually means.\n",
        "\n",
        "As we saw, we might lose potential information or insight because a lot of words make sense once they are put together. This problem can be solved by N-grams.\n",
        "\n",
        "N-grams are the fusion of multiple letters or multiple words. They are\n",
        "formed in such a way that even the previous and next words are captured.\n",
        "\n",
        "• Unigrams are the unique words present in the sentence. <br />\n",
        "• Bigram is the combination of 2 words. <br />\n",
        "• Trigram is 3 words and so on. <br />\n",
        "\n",
        "For example,\n",
        "“I am learning NLP”\n",
        "\n",
        "Unigrams: “I”, “am”, “ learning”, “NLP”\n",
        "\n",
        "Bigrams: “I am”, “am learning”, “learning NLP”\n",
        "\n",
        "Trigrams: “I am learning”, “am learning NLP”"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMzQTzuugcwQ"
      },
      "source": [
        "Problem\n",
        "--\n",
        "Generate the N-grams for the given sentence.\n",
        "\n",
        "Solution\n",
        "--\n",
        "There are a lot of packages that will generate the N-grams. The one that is\n",
        "mostly used is TextBlob."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "lf2EiS_ZgcwR",
        "outputId": "409049e7-2cf6-4ea5-dc06-bbaa5fe33455",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[WordList(['I']), WordList(['am']), WordList(['learning']), WordList(['NLP'])]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "Text = \"I am learning NLP\"\n",
        "\n",
        "# Use the below TextBlob function to create N-grams.\n",
        "# Use the text that is defined above\n",
        "# and mention the “n” based on the requirement.\n",
        "\n",
        "#Import textblob\n",
        "from textblob import TextBlob\n",
        "\n",
        "#For unigram : Use n = 1\n",
        "TextBlob(Text).ngrams(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "N3ZTQEu0gcwS",
        "outputId": "76d8db1a-9a2c-4c88-9eab-49adc63d1290",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[WordList(['I', 'am']),\n",
              " WordList(['am', 'learning']),\n",
              " WordList(['learning', 'NLP'])]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "#For Bigram : use n = 2\n",
        "TextBlob(Text).ngrams(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "s4uv8DclgcwT",
        "outputId": "25ba33d4-cd7c-4517-a5fe-f5101d473824",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[WordList(['I', 'am', 'learning']), WordList(['am', 'learning', 'NLP'])]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "#For trigram : use n = 3\n",
        "TextBlob(Text).ngrams(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4c7nkt_gcwT"
      },
      "source": [
        "Simple case study - to understand better\n",
        "--\n",
        "\n",
        "**input** :\n",
        "hello, thank you all for participating in the workshop. Participation in the workshop was worth it.\n",
        "\n",
        "**after lemma and basic text cleaning** :\n",
        "hello thank you all for participate in the workshop. Participate in the workshop was worth it.\n",
        "\n",
        "\n",
        "**n=4 BOW model of the above** :\n",
        "[hello thank you all], [thank you all for] , [you all for participate] ...\n",
        "[participate in the workshop], ... [Participate in the workshop], ...\n",
        "\n",
        "\n",
        "**Now do CV : count vect** :\n",
        "[participate in the workshop] -> 2\n",
        "and all other 4-gram phrases have a freqency of 1\n",
        "\n",
        "The context of this sentence is :\n",
        "[participate in the workshop]\n",
        "\n",
        "This brings out the importance of the document , which helps to classify the document.\n",
        "\n",
        "<hr />\n",
        "\n",
        "**Now Suppose , I give you 20 such sentences, and Ask which of them are similar ?**\n",
        "\n",
        "<br />\n",
        "Steps :\n",
        "1. Every sentence u would do the above.\n",
        "2. similar context would form a class of sentences.\n",
        "<br />\n",
        "for eg :\n",
        "Suppose sent_1 : has context as  [participate in the workshop]\n",
        "Suppose sent_9 : has context as  [fee for workshop participate]\n",
        "<br />\n",
        "hence both of the above sentences would be classified under one class.\n",
        "<br />\n",
        "Funda : Document Classification in NLP."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5VZ5FnLgcwU"
      },
      "source": [
        "Generating Bigram-based features for a document using CountVectorizer\n",
        "--\n",
        "\n",
        "Just like in the last code-example, we used TextBlob class, we can do the same thing by using count vectorizer to generate features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rMFEZpGSgcwU",
        "outputId": "bbb9473c-295f-49f9-df66-4c2a201b8d71",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'love': 7, 'nlp': 9, 'and': 1, 'will': 12, 'learn': 5, 'in': 3, '2month': 0, 'love nlp': 8, 'nlp and': 10, 'and will': 2, 'will learn': 13, 'learn nlp': 6, 'nlp in': 11, 'in 2month': 4}\n",
            "[[1 1 1 1 1 1 1 1 1 2 1 1 1 1]]\n"
          ]
        }
      ],
      "source": [
        "#importing the function\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Text\n",
        "text = [\"I love NLP and I will learn NLP in 2month\"]\n",
        "\n",
        "# create the transform\n",
        "vectorizer = CountVectorizer(ngram_range=(1,2))\n",
        "                            #ngram_range=(min,max)\n",
        "\n",
        "# tokenizing\n",
        "vectorizer.fit(text)\n",
        "\n",
        "# encode document\n",
        "vector = vectorizer.transform(text)\n",
        "\n",
        "# summarize & generating output\n",
        "print(vectorizer.vocabulary_)\n",
        "print(vector.toarray())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kiN7J90gcwV"
      },
      "source": [
        "The output has features with bigrams, and for our example, the count\n",
        "is one for all the tokens.\n",
        "\n",
        "**Note** : single letter words are not considered as \"words\" by the CountVectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKnD9UxzgcwW"
      },
      "source": [
        "Hash Vectorizing\n",
        "--\n",
        "CountVectorizer has one limitation. In this method, the vocabulary can\n",
        "become very large and cause memory/computation issues.\n",
        "\n",
        "One of the ways to solve this problem is a Hash Vectorizer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3uWpmNagcwW"
      },
      "source": [
        "Problem\n",
        "--\n",
        "Understand and generate a Hash Vectorizer.\n",
        "\n",
        "Solution\n",
        "--\n",
        "Hash Vectorizer is memory efficient and instead of storing the tokens\n",
        "as strings, the vectorizer applies the hashing trick to encode them as\n",
        "numerical indexes. The downside is that it’s one way and once vectorized,\n",
        "the features cannot be retrieved.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "KjThfXF0gcwW",
        "outputId": "d16c10c9-8b78-4c68-ed15-b6ac03c024f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 3)\n",
            "[[-0.81649658  0.40824829 -0.40824829]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "\n",
        "# list of text documents\n",
        "text = [\"The quick brown fox and the lazy dog.\"]\n",
        "\n",
        "# transform\n",
        "vectorizer = HashingVectorizer(n_features=3)\n",
        "# recommended reading : https://machinelearningmastery.com/prepare-text-data-machine-learning-scikit-learn/\n",
        "# https://datascience.stackexchange.com/questions/22250/what-is-the-difference-between-a-hashing-vectorizer-and-a-tfidf-vectorizer\n",
        "\n",
        "# create the hashing vector\n",
        "vector = vectorizer.transform(text)\n",
        "\n",
        "# summarize the vector\n",
        "print(vector.shape)\n",
        "print(vector.toarray())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvRc-WRvgcwX"
      },
      "source": [
        "It created vector of size __ and now this can be used for any\n",
        "supervised/unsupervised tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_jzqW5ogcwX"
      },
      "source": [
        "The trainer and participants should discuss the relevance of above vector o/p.\n",
        "\n",
        "References :\n",
        "\n",
        "Recommended reading :\n",
        "\n",
        "https://machinelearningmastery.com/prepare-text-data-machine-learning-scikit-learn/\n",
        "\n",
        "https://datascience.stackexchange.com/questions/22250/what-is-the-difference-between-a-hashing-vectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjewqSEUgcwY"
      },
      "source": [
        "Converting Text to Features Using TF-IDF\n",
        "--\n",
        "Again, in the above-mentioned text-to-feature methods, there are few\n",
        "drawbacks, hence the introduction of TF-IDF.\n",
        "\n",
        "Below are the disadvantages of the above methods.\n",
        "\n",
        "• Let’s say a particular word is appearing in all the documents of the corpus, then it will achieve higher importance in our previous methods. That’s bad for our analysis.\n",
        "\n",
        "• The whole idea of having TF-IDF is to reflect on how important a word is to a document in a collection, and hence normalizing words appeared frequently in all the documents.\n",
        "\n",
        "Problem\n",
        "--\n",
        "Text to feature using TF-IDF.\n",
        "\n",
        "Solution\n",
        "--\n",
        "Term frequency (TF): Term frequency is simply the ratio of the count of a\n",
        "word present in a sentence, to the length of the sentence.\n",
        "\n",
        "TF is basically capturing the importance of the word w.r.t the\n",
        "length of the document. For example, a word with the frequency of 3 with\n",
        "the length of sentence being 10 is not the same as when the word length of\n",
        "sentence is 100 words. It should get more importance in the first scenario;\n",
        "that is what TF does.\n",
        "\n",
        "Inverse Document Frequency (IDF): IDF of each word is the log of the ratio of the total number of rows to the number of rows in a particular document in which that word is present.\n",
        "\n",
        "IDF = log(N/n), where N is the total number of rows and n is the\n",
        "number of rows in which the word was present.\n",
        "\n",
        "IDF will measure the rareness of a term. Words like “a,” and “the” show\n",
        "up in all the documents of the corpus, but rare words will not be there\n",
        "in all the documents. So, if a word is appearing in almost all documents,\n",
        "then that word is of no use to us since it is not helping to classify or in\n",
        "information retrieval. IDF will nullify this problem.\n",
        "\n",
        "TF-IDF is the simple product of TF and IDF so that both of the drawbacks are addressed, which makes predictions and information retrieval relevant."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "KLmEIBg1gcwY",
        "outputId": "c152cfab-cd79-4474-efb1-2e24cdb63f78",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'the': 10, 'quick': 9, 'brown': 0, 'fox': 4, 'jumped': 6, 'over': 8, 'lazy': 7, 'dog': 1, 'is': 5, 'dummy': 2, 'example': 3}\n"
          ]
        }
      ],
      "source": [
        "Text = [\"The quick brown fox jumped over the lazy dog.\",\n",
        "\"The dog.\",\n",
        "\"The fox\",\n",
        "\"The is a dummy example\"]\n",
        "\n",
        "#Import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "#Create the TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "#Tokenize and build vocab\n",
        "vectorizer.fit(Text)\n",
        "\n",
        "#Summarize\n",
        "print(vectorizer.vocabulary_)\n",
        "#print(vectorizer.idf_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmHkCd0hgcwZ"
      },
      "source": [
        "If you observe, “the” is appearing in all the 3 documents and it does\n",
        "not add much value, and hence the vector value is 1, which is less than all\n",
        "the other vector representations of the tokens.\n",
        "\n",
        "All these methods or techniques we have looked into so far are based\n",
        "on frequency and hence called frequency-based embeddings or features.\n",
        "And in the next section, let us look at prediction-based embeddings,\n",
        "typically called word embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Fu_Dq_O5gcwZ",
        "outputId": "35443277-589c-496c-c073-2dc1c580a863",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 11)\n",
            "[[0.36929648 0.29115758 0.         0.         0.29115758 0.\n",
            "  0.36929648 0.36929648 0.36929648 0.36929648 0.38542844]]\n"
          ]
        }
      ],
      "source": [
        "# encode document\n",
        "vector = vectorizer.transform([\"The quick brown fox jumped over the lazy dog\"])\n",
        "\n",
        "# summarize encoded vector\n",
        "print(vector.shape)\n",
        "print(vector.toarray())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AD1g1BYDgcwa"
      },
      "source": [
        "Implementing Word Embeddings\n",
        "--\n",
        "This section assumes that you have a working knowledge of how a neural\n",
        "network works and you know terms like :\n",
        "\n",
        "a. Deep learning\n",
        "\n",
        "b. Perceptron and Sigmoid\n",
        "\n",
        "c. FFNN ( feed forward Neural Network)\n",
        "\n",
        "d. RNN (Recurrent Neural Network )\n",
        "\n",
        "**( If new to a Neural Network (NN), it is suggested that you go through Chapter 1 to gain a basic understanding of how NN works. )\n",
        "\n",
        "Even though all previous methods solve most of the problems, once we get into more complicated problems where we want to capture the semantic relation between the words, these methods fail to perform.\n",
        "\n",
        "Below are the challenges:\n",
        "\n",
        "• All these techniques fail to capture the context and meaning of the words. All the methods discussed so far basically depend on the appearance or frequency of the words. But we need to look at how to capture the context or semantic relations: that is, how frequently the words are appearing close by.\n",
        "\n",
        ">a. I am eating an apple.\n",
        "\n",
        ">b. I am using apple.\n",
        "\n",
        "If you observe the above example, Apple gives different meanings when it is used with different (close by) adjacent words, eating and using.\n",
        "\n",
        "• For a problem like a document classification (book classification in the library), a document is really huge and there are a humongous number of tokens\n",
        "generated. In these scenarios, your number of features can get out of control (wherein) thus hampering the accuracy and performance.\n",
        "\n",
        "A machine/algorithm can match two documents/texts and say whether they are same or not. But how do we make machines tell you about cricket or Virat Kohli when you search for MS Dhoni? How do you make a machine understand that “Apple” in “Apple is a tasty fruit” is a fruit that can be eaten and not a company?\n",
        "\n",
        "The answer to the above questions lies in creating a representation for words that capture their meanings, semantic relationships, and the different types of contexts they are used in.\n",
        "\n",
        "> The above challenges are addressed by Word Embeddings.\n",
        "\n",
        "Word embedding is the feature learning technique where words from the vocabulary are mapped to vectors of real numbers capturing the contextual hierarchy.\n",
        "\n",
        "If you observe the below table, every word is represented with 4 numbers called vectors. Using the word embeddings technique, we are going to derive those vectors for each and every word so that we can use it in future analysis. In the below example, the dimension is 4. But we usually use a dimension greater than 100."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14MbdhqRgcwb"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?id=165llWGYsReLC4BCtyZs6ZLYeggkg1k1m\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjvCd8i5gcwb"
      },
      "source": [
        "Problem\n",
        "--\n",
        "You want to implement word embeddings.\n",
        "\n",
        "Solution\n",
        "--\n",
        "Word embeddings are prediction based, and they use shallow neural networks to train the model that will lead to learning the weight and using them as a vector representation.\n",
        "\n",
        "word2vec\n",
        "--\n",
        "word2vec is the deep learning Google framework to train word embeddings. It will use all the words of the whole corpus and predict\n",
        "the nearby words. It will create a vector for all the words present in the\n",
        "corpus in a way so that the context is captured. It also outperforms any\n",
        "other methodologies in the space of word similarity and word analogies.\n",
        "\n",
        "There are mainly 2 types of word2vec Model.\n",
        "\n",
        "• Skip-Gram\n",
        "\n",
        "• Continuous Bag of Words (CBOW)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iQOYlTNg0uhY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQALqoALgcwb"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1ZC7kOYkuY2BGRCONWde38usTOCRJqJlR\" altext = \"word2vec_block_diagram\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCd2f8xagcwc"
      },
      "source": [
        "The above figure shows the architecture of the CBOW and skip-gram\n",
        "algorithms used to build word embeddings.\n",
        "\n",
        "CBOW vs Skip-Gram\n",
        "-------\n",
        "CBOW (Continuous Bag-Of-Words) is about creating a network that tries to predict the word in the middle given some surrounding words: [W[-3], W[-2], W[-1], W[1], W[2], W[3]] => W[0]\n",
        "\n",
        "Skip-Gram is the opposite of CBOW, try to predict the surrounding words given the word in the middle: W[0] => [W[-3], W[-2], W[-1], W[1], W[2], W[3]]\n",
        "\n",
        "\n",
        "Let us see how these models work in detail.\n",
        "\n",
        "Skip-Gram\n",
        "--\n",
        "The skip-gram model is used to predict the probabilities of a word given the context of word or words.\n",
        "\n",
        "Let us take a small sentence and understand how it actually works.\n",
        "Each sentence will generate a target word and context, which are the words\n",
        "nearby. The number of words to be considered around the target variable\n",
        "is called the window size. The table below shows all the possible target\n",
        "and context variables for window size 2. Window size needs to be selected\n",
        "based on data and the resources at your disposal. The larger the window\n",
        "size, the higher the computing power."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDdRvavVgcwc"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?id=18nKDL_JAX96Zs_ILGMrcdd517GWLwrW2\" altext = \"skipgram_output\">\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2ao_bougcwc"
      },
      "source": [
        "Since it takes a lot of text and computing power, let us go ahead and\n",
        "take sample data and build a skip-gram model.\n",
        "\n",
        "As mentioned in Part 3(of this Course), import the text corpus and break it into sentences. Perform some cleaning and preprocessing like the removal of\n",
        "punctuation and digits, and split the sentences into words or tokens, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "BRgDhW_fgcwd"
      },
      "outputs": [],
      "source": [
        "#Example sentences\n",
        "sentences = [['I', 'love', 'nlp'],\n",
        "['I', 'will', 'learn', 'nlp', 'in', '2','months'],\n",
        "['nlp', 'is', 'future'],\n",
        "[ 'nlp', 'saves', 'time', 'and', 'solves',\n",
        "'lot', 'of', 'industry', 'problems'],\n",
        "['nlp', 'uses', 'machine', 'learning']]\n",
        "\n",
        "#import library\n",
        "#!pip install gensim\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "#from sklearn.decomposition import PCA\n",
        "from matplotlib import pyplot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Ep4tRnkwgcwd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "216a9d59-aca7-40c1-c24e-5c007dde4a77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-1.0724545e-03  4.7286271e-04  1.0206699e-02  1.8018546e-02\n",
            " -1.8605899e-02 -1.4233618e-02  1.2917745e-02  1.7945977e-02\n",
            " -1.0030856e-02 -7.5267432e-03  1.4761009e-02 -3.0669428e-03\n",
            " -9.0732267e-03  1.3108104e-02 -9.7203208e-03 -3.6320353e-03\n",
            "  5.7531595e-03  1.9837476e-03 -1.6570430e-02 -1.8897636e-02\n",
            "  1.4623532e-02  1.0140524e-02  1.3515387e-02  1.5257311e-03\n",
            "  1.2701781e-02 -6.8107317e-03 -1.8928028e-03  1.1537147e-02\n",
            " -1.5043275e-02 -7.8722071e-03 -1.5023164e-02 -1.8600845e-03\n",
            "  1.9076237e-02 -1.4638334e-02 -4.6675373e-03 -3.8754821e-03\n",
            "  1.6154874e-02 -1.1861792e-02  9.0324880e-05 -9.5074680e-03\n",
            " -1.9207101e-02  1.0014586e-02 -1.7519170e-02 -8.7836506e-03\n",
            " -7.0199967e-05 -5.9236289e-04 -1.5322480e-02  1.9229487e-02\n",
            "  9.9641159e-03  1.8466286e-02]\n"
          ]
        }
      ],
      "source": [
        "# training the model\n",
        "skipgram = Word2Vec(sentences, vector_size=50, window = 3, min_count=1, sg=1)\n",
        "# vector_size=50 -> means size of vector to represent each token or word (default 100)\n",
        "# window=3 -> The maximum distance between the target word and its neighboring word.(default 5)\n",
        "# min_count=1 -> Minimium frequency count of words.\n",
        "#                The model would ignore words that do not satisfy the min_count.\n",
        "#                Extremely infrequent words are usually unimportant. (default 5)\n",
        "# workers -> How many threads to use behind the scenes? (default 3)\n",
        "# sg -> (default 0 or CBOW) The training algorithm, either CBOW (0)\n",
        "#                           or skip gram (1).\n",
        "\n",
        "\n",
        "# access vector for one word\n",
        "print(skipgram.wv['nlp'])\n",
        "## skipgram.wv['nlp'] accesses the word vector for the word 'nlp' in the Word2Vec model skipgram.\n",
        "\n",
        "# Since our vector size parameter was 50, the model\n",
        "# gives a vector of size 50 for each word."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Huwtav-egcwe"
      },
      "source": [
        "Recommended reading :\n",
        "--\n",
        "https://medium.freecodecamp.org/how-to-get-started-with-word2vec-and-then-how-to-make-it-work-d0a2fca9dad3\n",
        "\n",
        "https://nlpforhackers.io/word-embeddings/\n",
        "\n",
        "https://machinelearningmastery.com/develop-word-embeddings-python-gensim/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "nzgII11bgcwe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5404a34c-5cef-45fe-85d3-6ec7cd1e6648"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-1.0724545e-03  4.7286271e-04  1.0206699e-02  1.8018546e-02\n",
            " -1.8605899e-02 -1.4233618e-02  1.2917745e-02  1.7945977e-02\n",
            " -1.0030856e-02 -7.5267432e-03  1.4761009e-02 -3.0669428e-03\n",
            " -9.0732267e-03  1.3108104e-02 -9.7203208e-03 -3.6320353e-03\n",
            "  5.7531595e-03  1.9837476e-03 -1.6570430e-02 -1.8897636e-02\n",
            "  1.4623532e-02  1.0140524e-02  1.3515387e-02  1.5257311e-03\n",
            "  1.2701781e-02 -6.8107317e-03 -1.8928028e-03  1.1537147e-02\n",
            " -1.5043275e-02 -7.8722071e-03 -1.5023164e-02 -1.8600845e-03\n",
            "  1.9076237e-02 -1.4638334e-02 -4.6675373e-03 -3.8754821e-03\n",
            "  1.6154874e-02 -1.1861792e-02  9.0324880e-05 -9.5074680e-03\n",
            " -1.9207101e-02  1.0014586e-02 -1.7519170e-02 -8.7836506e-03\n",
            " -7.0199967e-05 -5.9236289e-04 -1.5322480e-02  1.9229487e-02\n",
            "  9.9641159e-03  1.8466286e-02]\n"
          ]
        }
      ],
      "source": [
        "# access vector for another word\n",
        "print(skipgram.wv['nlp'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QNViDkFgcwf"
      },
      "source": [
        "Note : We get an error saying the word doesn’t exist because this word was\n",
        "not there in our input training data. This is the reason we need to train the\n",
        "algorithm on as much data possible so that we do not miss out on words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHDFriIJgcwf"
      },
      "source": [
        "Continuous Bag of Words (CBOW)\n",
        "--\n",
        "Now let’s see how to build CBOW model. (Its very similar to SkipGram model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "DsNpCVmEgcwg"
      },
      "outputs": [],
      "source": [
        "#import library\n",
        "from gensim.models import Word2Vec\n",
        "#from sklearn.decomposition import PCA\n",
        "from matplotlib import pyplot\n",
        "\n",
        "#Example sentences\n",
        "sentences = [['I', 'love', 'nlp'],\n",
        "['I', 'will', 'learn', 'nlp', 'in', '2','months'],\n",
        "['nlp', 'is', 'future'],\n",
        "[ 'nlp', 'saves', 'time', 'and', 'solves',\n",
        "'lot', 'of', 'industry', 'problems'],\n",
        "['nlp', 'uses', 'machine', 'learning']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "vgRYm2eTgcwg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "711b0be2-8a72-4d39-aa7e-2f4c22e5f450"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-1.0724545e-03  4.7286271e-04  1.0206699e-02  1.8018546e-02\n",
            " -1.8605899e-02 -1.4233618e-02  1.2917745e-02  1.7945977e-02\n",
            " -1.0030856e-02 -7.5267432e-03  1.4761009e-02 -3.0669428e-03\n",
            " -9.0732267e-03  1.3108104e-02 -9.7203208e-03 -3.6320353e-03\n",
            "  5.7531595e-03  1.9837476e-03 -1.6570430e-02 -1.8897636e-02\n",
            "  1.4623532e-02  1.0140524e-02  1.3515387e-02  1.5257311e-03\n",
            "  1.2701781e-02 -6.8107317e-03 -1.8928028e-03  1.1537147e-02\n",
            " -1.5043275e-02 -7.8722071e-03 -1.5023164e-02 -1.8600845e-03\n",
            "  1.9076237e-02 -1.4638334e-02 -4.6675373e-03 -3.8754821e-03\n",
            "  1.6154874e-02 -1.1861792e-02  9.0324880e-05 -9.5074680e-03\n",
            " -1.9207101e-02  1.0014586e-02 -1.7519170e-02 -8.7836506e-03\n",
            " -7.0199967e-05 -5.9236289e-04 -1.5322480e-02  1.9229487e-02\n",
            "  9.9641159e-03  1.8466286e-02]\n"
          ]
        }
      ],
      "source": [
        "# training the model\n",
        "cbow = Word2Vec(sentences, vector_size=50, window = 3, min_count=1, sg=0)\n",
        "# vector_size=50 -> means size of vector to represent each token or word\n",
        "# window=1 -> The maximum distance between the target word and its neighboring word.\n",
        "# min_count=1 -> Minimium frequency count of words.\n",
        "#                The model would ignore words that do not satisfy the min_count.\n",
        "#                Extremely infrequent words are usually unimportant.\n",
        "# workers -> How many threads to use behind the scenes?\n",
        "# as sg=0 i.e no skipgram , hence default CBOW\n",
        "\n",
        "\n",
        "# access vector for one word\n",
        "print(cbow.wv['nlp'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8T7qJSjxgcwg"
      },
      "source": [
        "Important Observation\n",
        "--\n",
        "To train these models, it requires a huge amount of computing\n",
        "power. So, let us go ahead and use Google’s pre-trained model, which has\n",
        "been trained with over 100 billion words.\n",
        "\n",
        "<font color='red'> <u>Note</u> : The Google Db is soo large that we may get ValueError, like this :  ValueError: array is too big; `arr.size * arr.dtype.itemsize` is larger than the maximum possible size. </font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "5vQxwmY9gcwh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee66b596-22a0-4c4b-d6a3-7ebef0706189"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
          ]
        }
      ],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "# import gensim package\n",
        "import gensim\n",
        "\n",
        "# load the saved model\n",
        "#model = gensim.models.KeyedVectors.load_word2vec_format('datasets/GoogleNews-vectors-negative300.bin', binary=True)\n",
        "import gensim.downloader as api\n",
        "wv = api.load('word2vec-google-news-300')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "uwMsRUkugcwh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b820f3ea-014d-4042-aa19-59ed64f7383f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7834683\n"
          ]
        }
      ],
      "source": [
        "#Checking how similarity works.\n",
        "print (wv.similarity('her', 'she'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "CuTRmFaAgcwh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47e1e6bc-bd64-4af8-f3d3-3b412c20af6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.12112989\n"
          ]
        }
      ],
      "source": [
        "#Lets check one more.\n",
        "print (wv.similarity('pizza', 'water'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4whZxc6Fgcwi"
      },
      "source": [
        "> Analysis on similarity :\n",
        "\n",
        "“This” and “is” has some similarity ( around 40 %), but the similarity\n",
        "between the words “post” and “book” is poor ( just 5 %). For any given set of words, it uses the vectors of both the words and calculates the similarity between them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "oW6I9YvOgcwi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "f27dee30-a1bb-483a-e343-8fda43a88bf9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cereal'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "# Finding the odd one out.\n",
        "wv.doesnt_match('breakfast cereal dinner lunch'.split())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFXhJIZkgcwj"
      },
      "source": [
        "Of 'breakfast’, ‘cereal’, ‘dinner’ and ‘lunch', only cereal is the word that is\n",
        "not anywhere related to the remaining 3 words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "Q8hozaydgcwj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e30cfd36-abaa-49f6-fef3-e1f081b6295a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('queen', 0.7118193507194519),\n",
              " ('monarch', 0.6189674139022827),\n",
              " ('princess', 0.5902431011199951),\n",
              " ('crown_prince', 0.5499460697174072),\n",
              " ('prince', 0.5377321839332581),\n",
              " ('kings', 0.5236844420433044),\n",
              " ('Queen_Consort', 0.5235945582389832),\n",
              " ('queens', 0.5181134343147278),\n",
              " ('sultan', 0.5098593831062317),\n",
              " ('monarchy', 0.5087411999702454)]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "# It is also finding the relations between words.\n",
        "wv.most_similar(positive=['woman', 'king'],negative=['man'])\n",
        "# default value of topn is 10\n",
        "\n",
        "# try this too :\n",
        "# model.most_similar(positive=['woman', 'king'],negative=['man'], topn=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ircG1GBwgcwj"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?id=11Yu1Gj4Rw5BccL6KXnT_rXqYPyJbEUfZ\" altext = \"word2Vec_google_sample_output_image\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jS32oBsEgcwk"
      },
      "source": [
        "Implementing fastText\n",
        "--\n",
        "fastText is another deep learning framework developed by Facebook to\n",
        "capture context and meaning.\n",
        "\n",
        "Problem\n",
        "--\n",
        "How to implement fastText in Python.\n",
        "\n",
        "Solution\n",
        "--\n",
        "fastText is the improvised version of word2vec. word2vec basically\n",
        "considers words to build the representation. But fastText takes each\n",
        "character while computing the representation of the word."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import FastText\n",
        "from gensim.models import FastText\n",
        "\n",
        "# Example sentences related to food ingredients and recipes\n",
        "sentences = [\n",
        "    ['pizza', 'toppings', 'include', 'cheese', 'tomato', 'pepperoni'],\n",
        "    ['spaghetti', 'sauce', 'ingredients', 'include', 'tomato', 'garlic', 'onion'],\n",
        "    ['chocolate', 'chip', 'cookie', 'recipe', 'calls', 'for', 'butter', 'flour', 'sugar', 'chocolate'],\n",
        "    ['chicken', 'noodle', 'soup', 'requires', 'chicken', 'noodles', 'carrots', 'celery', 'broth'],\n",
        "    ['vegetarian', 'taco', 'recipe', 'includes', 'black', 'beans', 'corn', 'bell', 'peppers', 'avocado']\n",
        "]\n",
        "\n",
        "# Train the FastText model\n",
        "fast = FastText(sentences, vector_size=10, window=3, min_count=2, workers=5, min_n=1, max_n=2)\n",
        "# vector_size=10 -> means size of vector to represent each token or word\n",
        "# window=1 -> The maximum distance between the target word and its neighboring word.\n",
        "# min_count=1 -> Minimium frequency count of words.\n",
        "#                The model would ignore words that do not satisfy the min_count.\n",
        "#                Extremely infrequent words are usually unimportant.\n",
        "# workers -> How many threads to use behind the scenes?\n",
        "# min_n=1, max_n=2  -> means model will consider both unigrams and bigrams.\n",
        "# By default, min_n=3 and max_n=6 in the FastText model.\n",
        "\n",
        "# Perform vector arithmetic to find a word analogy using the FastText model\n",
        "similar_words = fast.wv.most_similar(positive=['cheese', 'bread'], negative=['pizza'], topn=3)\n",
        "## we perform vector arithmetic to find a word analogy. In this case, we're looking\n",
        "## for words similar to \"cheese\" and \"bread\" but not similar to \"pizza\".\n",
        "## This can represent a query like \"What goes well with bread, other than pizza?\"\n",
        "\n",
        "print(similar_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVdc5K3sMUJs",
        "outputId": "98adfe6c-1023-4afd-9602-2e436c552fb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('recipe', 0.7209869027137756), ('chicken', 0.4884187579154968), ('chocolate', -0.04585491493344307)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "OVVrBtYegcwl"
      },
      "source": [
        "We hope that by now you are familiar and comfortable with processing\n",
        "the natural language. Now that data is cleaned and features are created,\n",
        "let’s jump into building some applications around it that solves the\n",
        "business problem."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's a use case that demonstrates the usage of FastText for building a text classification model:\n",
        "\n",
        "### Use Case: Text Classification for Customer Reviews\n",
        "\n",
        "#### Problem Statement:\n",
        "An e-commerce company wants to classify customer reviews into different categories (e.g., positive, neutral, negative) to gain insights into customer sentiment and improve their products and services.\n",
        "\n",
        "#### Solution with FastText:\n",
        "We can use FastText to build a text classification model that analyzes customer reviews and predicts the sentiment category.\n",
        "\n",
        "#### Steps:\n",
        "\n",
        "1. **Data Collection**: Collect a dataset of customer reviews along with their sentiment labels (e.g., positive, neutral, negative).\n",
        "\n",
        "2. **Data Preprocessing**: Preprocess the text data by removing stopwords, punctuation, and converting text to lowercase. Tokenization and lemmatization can also be applied.\n",
        "\n",
        "3. **Feature Extraction**: Train a FastText model on the preprocessed text data to learn word embeddings. These embeddings capture semantic information about words and their contexts.\n",
        "\n",
        "4. **Model Training**: Use the word embeddings learned by FastText to represent each review as a vector by averaging the embeddings of its constituent words. Train a classification model (e.g., logistic regression, support vector machine) on these vector representations to predict the sentiment category.\n",
        "\n",
        "5. **Model Evaluation**: Evaluate the performance of the trained model using appropriate evaluation metrics such as accuracy, precision, recall, and F1-score on a held-out test set.\n",
        "\n",
        "6. **Deployment**: Deploy the trained model into production to classify new customer reviews in real-time. Monitor the model's performance and fine-tune as necessary.\n",
        "\n",
        "#### Benefits of Using FastText:\n",
        "\n",
        "- **Efficiency**: FastText is known for its efficiency in training and inference, making it suitable for large-scale text classification tasks.\n",
        "  \n",
        "- **Semantic Information**: FastText captures semantic relationships between words, allowing the model to generalize well to unseen words and contexts.\n",
        "\n",
        "- **Robustness**: FastText can handle out-of-vocabulary words, misspellings, and morphologically rich languages effectively, enhancing the robustness of the text classification model.\n",
        "\n",
        "- **Interpretability**: The word embeddings learned by FastText provide interpretable representations of words, facilitating the understanding of the model's predictions.\n",
        "\n",
        "#### Conclusion:\n",
        "By leveraging FastText for text classification, the e-commerce company can effectively analyze customer reviews, gain insights into customer sentiment, and make data-driven decisions to enhance customer satisfaction and improve their products and services."
      ],
      "metadata": {
        "id": "WM4NqgmkQxgK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import FastText\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "\n",
        "# Sample customer reviews on clothes and electronic products\n",
        "clothes_reviews = [\n",
        "    \"The dress is beautiful and fits perfectly.\",\n",
        "    \"The fabric of the shirt is soft and comfortable.\",\n",
        "    \"I love the design of these jeans.\",\n",
        "    \"The quality of the sweater is excellent.\",\n",
        "    \"The skirt is too tight and uncomfortable.\",\n",
        "]\n",
        "\n",
        "electronic_reviews = [\n",
        "    \"The phone is fast and has a great camera.\",\n",
        "    \"The laptop has a long battery life and works smoothly.\",\n",
        "    \"I'm impressed with the performance of the tablet.\",\n",
        "    \"The sound quality of the headphones is amazing.\",\n",
        "    \"The screen of the monitor is clear and sharp.\",\n",
        "]\n",
        "\n",
        "# Labels for the reviews (0: clothes, 1: electronics)\n",
        "labels = [0] * len(clothes_reviews) + [1] * len(electronic_reviews)\n",
        "\n",
        "# Combine clothes and electronic reviews\n",
        "all_reviews = clothes_reviews + electronic_reviews\n",
        "\n",
        "# Preprocessing function to tokenize, remove stopwords, punctuation, and lemmatize the text data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words and word.isalpha()]\n",
        "    return tokens\n",
        "\n",
        "# Train FastText model on the preprocessed text data\n",
        "fasttext_model = FastText([preprocess(review) for review in all_reviews], vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Convert each review into a vector representation using the trained FastText model\n",
        "def review_to_vector(review):\n",
        "    vectors = [fasttext_model.wv[word] for word in preprocess(review) if word in fasttext_model.wv]\n",
        "    if vectors:\n",
        "        return np.mean(vectors, axis=0)\n",
        "    else:\n",
        "        return np.zeros(fasttext_model.vector_size)\n",
        "\n",
        "review_vectors = np.array([review_to_vector(review) for review in all_reviews])\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(review_vectors, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Random Forest classifier on the vector representations of the reviews\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = rf_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=[\"Clothes\", \"Electronics\"]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bGcrdzhUJBPw",
        "outputId": "5f6c5850-f236-4dc7-a402-5184bdd321c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Clothes       1.00      1.00      1.00         1\n",
            " Electronics       1.00      1.00      1.00         1\n",
            "\n",
            "    accuracy                           1.00         2\n",
            "   macro avg       1.00      1.00      1.00         2\n",
            "weighted avg       1.00      1.00      1.00         2\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logical explanation of the code:\n",
        "\n",
        "1. **Data Preparation**:\n",
        "   - We start by defining sample customer reviews for clothes and electronic products, along with their corresponding labels.\n",
        "   - These reviews are then combined into a single list, `all_reviews`, along with their labels.\n",
        "\n",
        "2. **Preprocessing**:\n",
        "   - We perform text preprocessing on the reviews before training the model.\n",
        "   - The `preprocess` function tokenizes each review, removes stopwords, punctuation, and performs lemmatization to normalize the text data.\n",
        "\n",
        "3. **FastText Model Training**:\n",
        "   - We train a FastText model on the preprocessed text data using Gensim.\n",
        "   - FastText learns vector representations (embeddings) for each word in the vocabulary, capturing semantic meanings of words based on their context in the reviews.\n",
        "\n",
        "4. **Feature Extraction**:\n",
        "   - For each review, we use the trained FastText model to convert the text into a fixed-length vector representation.\n",
        "   - The `review_to_vector` function computes the average vector of all words in the review, excluding stopwords and non-alphabetic tokens.\n",
        "\n",
        "5. **Train-Test Split**:\n",
        "   - We split the data into training and testing sets using `train_test_split` from scikit-learn.\n",
        "   - This ensures that the model is trained on a portion of the data and evaluated on a separate unseen portion.\n",
        "\n",
        "6. **Model Training (Random Forest)**:\n",
        "   - We use a Random Forest classifier to train the model on the vector representations of the reviews.\n",
        "   - Random Forest is an ensemble learning method that builds multiple decision trees during training and outputs the mode (or average) prediction of the individual trees.\n",
        "\n",
        "7. **Model Evaluation**:\n",
        "   - We make predictions on the test set using the trained model.\n",
        "   - Accuracy is calculated as the proportion of correctly classified samples out of the total number of samples.\n",
        "   - The classification report provides precision, recall, and F1-score for each class (Clothes and Electronics), along with their average values.\n",
        "\n",
        "By following these steps, the code aims to train a model that can accurately classify customer reviews as either related to clothes or electronic products based on their text content."
      ],
      "metadata": {
        "id": "Wz_z4qF9URxG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Home work - Use case**\n",
        "\n",
        "Use Continuous Bag of Words (CBOW) Word2Vec model for sentiment analysis on movie reviews:\n",
        "\n",
        "### Use Case: Movie Review Sentiment Analysis\n",
        "\n",
        "**Problem Statement**:\n",
        "We want to classify movie reviews as positive or negative based on their sentiment.\n",
        "\n",
        "**Dataset**:\n",
        "Use the IMDb movie review dataset, which contains labeled movie reviews as positive or negative.\n",
        "\n",
        "**Approach**:\n",
        "1. **Data Preparation**:\n",
        "   - Download and preprocess the IMDb movie review dataset. ( source : kaggle.com )\n",
        "   - Split the dataset into training and testing sets.\n",
        "\n",
        "2. **Word Embedding (CBOW Word2Vec)**:\n",
        "   - Train a CBOW Word2Vec model on the preprocessed movie review text.\n",
        "   - CBOW model predicts the target word (current word) based on the context words (surrounding words) within a fixed-size window.\n",
        "\n",
        "3. **Feature Extraction**:\n",
        "   - For each movie review, compute the average vector representation of all words using the trained CBOW Word2Vec model.\n",
        "   - These average vectors serve as features for sentiment analysis.\n",
        "\n",
        "4. **Model Training**:\n",
        "   - Train a machine learning model (e.g., Logistic Regression, Random Forest, or Support Vector Machine) on the feature vectors extracted from the movie reviews.\n",
        "\n",
        "5. **Model Evaluation**:\n",
        "   - Evaluate the trained model on the testing set to measure its performance in classifying movie reviews as positive or negative.\n",
        "\n",
        "**Code Implementation** (using Python and Gensim):\n",
        "```python\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import numpy as np\n",
        "\n",
        "# Load and preprocess the IMDb movie review dataset\n",
        "\n",
        "# Train CBOW Word2Vec model on preprocessed text data\n",
        "cbow_model = Word2Vec(sentences, vector_size=50, window=3, min_count=1, sg=0)\n",
        "\n",
        "# Convert each movie review into a vector representation using the trained CBOW Word2Vec model\n",
        "\n",
        "# Split the data into train and test sets\n",
        "\n",
        "# Train a machine learning model (e.g., Logistic Regression) on the feature vectors\n",
        "\n",
        "# Evaluate the trained model on the testing set\n",
        "\n",
        "# Print accuracy and classification report\n",
        "```\n",
        "\n",
        "This use case demonstrates how to leverage the CBOW Word2Vec model for sentiment analysis on movie reviews. By learning vector representations of words in the context of movie reviews, the model can capture semantic meanings and relationships, enabling accurate sentiment classification."
      ],
      "metadata": {
        "id": "3uUi87P1U5PQ"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}